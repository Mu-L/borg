# cython: language_level=3

API_VERSION = '1.2_01'

import os
import errno
import time

from ..constants import CH_DATA, CH_ALLOC, CH_HOLE, zeros
from .reader import FileReader, Chunk


class ChunkerFailing:
    """
    This is a very simple chunker for testing purposes.

    Reads block_size chunks, starts failing at block <fail_start>, <fail_count> failures, then succeeds.
    """
    def __init__(self, block_size, map):
        self.block_size = block_size
        # one char per block: r/R = successful read, e/E = I/O Error, e.g.: "rrrrErrrEEr"
        # blocks beyond the map will have same behaviour as the last map char indicates.
        map = map.upper()
        if not set(map).issubset({"R", "E"}):
            raise ValueError("unsupported map character")
        self.map = map
        self.count = 0
        self.chunking_time = 0.0  # not updated, just provided so that caller does not crash

    def chunkify(self, fd=None, fh=-1):
        """
        Cut a file into chunks.

        :param fd: Python file object
        :param fh: OS-level file handle (if available),
                   defaults to -1 which means not to use OS-level fd.
        """
        use_fh = fh >= 0
        wanted = self.block_size
        while True:
            data = os.read(fh, wanted) if use_fh else fd.read(wanted)
            got = len(data)
            if got > 0:
                idx = self.count if self.count < len(self.map) else -1
                behaviour = self.map[idx]
                if behaviour == "E":
                    self.count += 1
                    fname = None if use_fh else getattr(fd, "name", None)
                    raise OSError(errno.EIO, "simulated I/O error", fname)
                elif behaviour == "R":
                    self.count += 1
                    yield Chunk(data, size=got, allocation=CH_DATA)
                else:
                    raise ValueError("unsupported map character")
            if got < wanted:
                # we did not get enough data, looks like EOF.
                return


class ChunkerFixed:
    """
    This is a simple chunker for input data with data usually staying at same
    offset and / or with known block/record sizes:

    - raw disk images
    - block devices
    - database files with simple header + fixed-size records layout

    It optionally supports:

    - a header block of different size
    - using a sparsemap to read only data ranges and seek over hole ranges
      for sparse files.
    - using an externally given filemap to read only specific ranges from
      a file.

    Note: the last block of a data or hole range may be less than the block size,
          this is supported and not considered to be an error.
    """
    def __init__(self, block_size, header_size=0, sparse=False):
        self.block_size = block_size
        self.header_size = header_size
        self.chunking_time = 0.0  # likely will stay close to zero - not much to do here.
        self.reader_block_size = 1024 * 1024
        self.reader = None
        self.sparse = sparse

    def chunkify(self, fd=None, fh=-1, fmap=None):
        """
        Cut a file into chunks.

        :param fd: Python file object
        :param fh: OS-level file handle (if available),
                   defaults to -1 which means not to use OS-level fd.
        :param fmap: a file map, same format as generated by sparsemap
        """
        # Initialize the reader with the file descriptors
        self.reader = FileReader(fd=fd, fh=fh, read_size=self.reader_block_size,
                                sparse=self.sparse, fmap=fmap)

        # Handle header if present
        if self.header_size > 0:
            # Read the header block using read
            started_chunking = time.monotonic()
            header_chunk = self.reader.read(self.header_size)
            self.chunking_time += time.monotonic() - started_chunking

            if header_chunk.meta["size"] > 0:
                # Yield the header chunk
                yield header_chunk

        # Process the rest of the file using read
        while True:
            started_chunking = time.monotonic()
            chunk = self.reader.read(self.block_size)
            self.chunking_time += time.monotonic() - started_chunking
            size = chunk.meta["size"]
            if size == 0:
                break  # EOF
            assert size <= self.block_size
            yield chunk
